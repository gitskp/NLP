{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Pre-Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tokenization\n",
    "- Tagging\n",
    "- Chunking\n",
    "- Stemming\n",
    "- Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Tokenization\n",
    "Sentence tokenization is the process of splitting a text corpus into sentences that act as\n",
    "the first level of tokens which the corpus is comprised of. This is also known as sentence\n",
    "segmentation\n",
    "\n",
    "We will primarily focus on the following sentence tokenizers:\n",
    "- sent_tokenize\n",
    "- PunktSentenceTokenizer\n",
    "- RegexpTokenizer\n",
    "- Pre-trained sentence tokenization models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "from pprint import pprint\n",
    "\n",
    "alice = gutenberg.raw(fileids='carroll-alice.txt')\n",
    "sample_text = \" We will discuss briefly about the basic syntax, structure and design philosophies. There is a defined hierarchical syntax for Python code which you should remember when writing code! Python is a really powerful programming language!\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144395\n"
     ]
    }
   ],
   "source": [
    "# Total characters in Alice in Wonderland\n",
    "print( len(alice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Alice's Adventures in Wonderland by Lewis Carroll 1865]\n",
      "\n",
      "CHAPTER I. Down the Rabbit-Hole\n",
      "\n",
      "Alice was\n"
     ]
    }
   ],
   "source": [
    "# First 100 characters in the corpus\n",
    "print(alice[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nltk.sent_tokenize function is the default sentence tokenization function that\n",
    "nltk recommends. It uses an instance of the PunktSentenceTokenizer class internally.\n",
    "However, this is not just a normal object or instance of that class—it has been pre-trained\n",
    "on several language models and works really well on many popular languages besides\n",
    "just English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences in sample_text: 3\n",
      "Sample text sentences :-\n",
      "[' We will discuss briefly about the basic syntax, structure and design '\n",
      " 'philosophies.',\n",
      " 'There is a defined hierarchical syntax for Python code which you should '\n",
      " 'remember when writing code!',\n",
      " 'Python is a really powerful programming language!']\n",
      "\n",
      "Total sentences in alice: 1625\n",
      "First 5 sentences in alice:-\n",
      "[\"[Alice's Adventures in Wonderland by Lewis Carroll 1865]\\n\\nCHAPTER I.\",\n",
      " 'Down the Rabbit-Hole\\n'\n",
      " '\\n'\n",
      " 'Alice was beginning to get very tired of sitting by her sister on the\\n'\n",
      " 'bank, and of having nothing to do: once or twice she had peeped into the\\n'\n",
      " 'book her sister was reading, but it had no pictures or conversations in\\n'\n",
      " \"it, 'and what is the use of a book,' thought Alice 'without pictures or\\n\"\n",
      " \"conversation?'\",\n",
      " 'So she was considering in her own mind (as well as she could, for the\\n'\n",
      " 'hot day made her feel very sleepy and stupid), whether the pleasure\\n'\n",
      " 'of making a daisy-chain would be worth the trouble of getting up and\\n'\n",
      " 'picking the daisies, when suddenly a White Rabbit with pink eyes ran\\n'\n",
      " 'close by her.',\n",
      " 'There was nothing so VERY remarkable in that; nor did Alice think it so\\n'\n",
      " \"VERY much out of the way to hear the Rabbit say to itself, 'Oh dear!\",\n",
      " 'Oh dear!']\n"
     ]
    }
   ],
   "source": [
    "default_st = nltk.sent_tokenize\n",
    "alice_sentences = default_st(text=alice)\n",
    "sample_sentences = default_st(text=sample_text)\n",
    "print('Total sentences in sample_text:', len(sample_sentences))\n",
    "print('Sample text sentences :-')\n",
    "pprint(sample_sentences)\n",
    "print('\\nTotal sentences in alice:', len(alice_sentences))\n",
    "print('First 5 sentences in alice:-')\n",
    "pprint(alice_sentences[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157171\n",
      " \n",
      "Wiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sit\n"
     ]
    }
   ],
   "source": [
    "# loading a German text corpus and inspecting it:\n",
    "\n",
    "from nltk.corpus import europarl_raw\n",
    "\n",
    "german_text = europarl_raw.german.raw(fileids='ep-00-01-17.de')\n",
    "# Total characters in the corpus\n",
    "print (len(german_text))\n",
    "# First 100 characters in the corpus\n",
    "print (german_text[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.tokenize.punkt.PunktSentenceTokenizer'>\n"
     ]
    }
   ],
   "source": [
    "german_sentences_def = default_st(text=german_text,language='german')\n",
    "\n",
    "# loading german text tokenizer into a PunktSentenceTokenizer instance\n",
    "german_tokenizer = nltk.data.load(resource_url='tokenizers/punkt/german.pickle')\n",
    "german_sentences = german_tokenizer.tokenize(german_text)\n",
    "\n",
    "# verify the type of german_tokenizer\n",
    "# should be PunktSentenceTokenizer\n",
    "print (type(german_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      " \n",
      "Wiederaufnahme der Sitzungsperiode Ich erkläre die am Freitag , dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen , wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe , daß Sie schöne Ferien hatten .\n",
      "Wie Sie feststellen konnten , ist der gefürchtete \" Millenium-Bug \" nicht eingetreten .\n",
      "Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden .\n",
      "Im Parlament besteht der Wunsch nach einer Aussprache im Verlauf dieser Sitzungsperiode in den nächsten Tagen .\n",
      "Heute möchte ich Sie bitten - das ist auch der Wunsch einiger Kolleginnen und Kollegen - , allen Opfern der Stürme , insbesondere in den verschiedenen Ländern der Europäischen Union , in einer Schweigeminute zu gedenken .\n"
     ]
    }
   ],
   "source": [
    "print (german_sentences_def == german_sentences)\n",
    " # print first 5 sentences of the corpus\n",
    "for sent in german_sentences[0:5]:\n",
    "    print (sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' We will discuss briefly about the basic syntax, structure and design '\n",
      " 'philosophies.',\n",
      " 'There is a defined hierarchical syntax for Python code which you should '\n",
      " 'remember when writing code!',\n",
      " 'Python is a really powerful programming language!']\n"
     ]
    }
   ],
   "source": [
    "punkt_st = nltk.tokenize.PunktSentenceTokenizer()\n",
    "sample_sentences = punkt_st.tokenize(sample_text)\n",
    "pprint(sample_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' We will discuss briefly about the basic syntax, structure and design '\n",
      " 'philosophies.',\n",
      " 'There is a defined hierarchical syntax for Python code which you should '\n",
      " 'remember when writing code!',\n",
      " 'Python is a really powerful programming language!']\n"
     ]
    }
   ],
   "source": [
    "SENTENCE_TOKENS_PATTERN = r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<![A-Z]\\.)(?<=\\.|\\?|\\!)\\s'\n",
    "regex_st = nltk.tokenize.RegexpTokenizer(pattern=SENTENCE_TOKENS_PATTERN,gaps=True)\n",
    "sample_sentences = regex_st.tokenize(sample_text)\n",
    "pprint(sample_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Tokenization\n",
    "Word tokenization is the process of splitting or segmenting sentences into their\n",
    "constituent words. A sentence is a collection of words, and with tokenization we\n",
    "essentially split a sentence into a list of words that can be used to reconstruct the\n",
    "sentence. Word tokenization is very important in many processes, especially in cleaning\n",
    "and normalizing text where operations like stemming and lemmatization work on\n",
    "each individual word based on its respective stems and lemma.\n",
    "- word_tokenize\n",
    "- TreebankWordTokenizer\n",
    "- RegexpTokenizer\n",
    "- Inherited tokenizers from RegexpTokenizer<br>\n",
    "The nltk. word_tokenize function is the default and recommended word tokenizer as specified\n",
    "by nltk . This tokenizer is actually an instance or object of the TreebankWordTokenizer\n",
    "class in its internal implementation and acts as a wrapper to that core class. The following\n",
    "snippet illustrates its usage:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', 'was', \"n't\", 'that', 'quick', 'and', 'he', 'could', \"n't\", 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The brown fox wasn't that quick and he couldn't win the race\"\n",
    "\n",
    "default_wt = nltk.word_tokenize\n",
    "words = default_wt(sentence)\n",
    "print (words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TreebankWordTokenizer is based on the Penn Treebank and uses various regular\n",
    "expressions to tokenize the text.\n",
    "- Splits and separates out periods that appear at the end of a sentence\n",
    "- Splits and separates commas and single quotes when followed by whitespaces\n",
    "- Most punctuation characters are split and separated into independent tokens\n",
    "- Splits words with standard contractions—examples would be don’t to do and n’t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', 'was', \"n't\", 'that', 'quick', 'and', 'he', 'could', \"n't\", 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "treebank_wt = nltk.TreebankWordTokenizer()\n",
    "words = treebank_wt.tokenize(sentence)\n",
    "print (words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word_tokenize() and treebankWordTokenizer() follow same tokenixe machnism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RegexpTokenizer\n",
    "RegexpTokenizer class to tokenize sentences into words. Remember, there are two main parameters that are useful\n",
    "in tokenization: the regex pattern for building the tokenizer and the gaps parameter,\n",
    "which, if set to True , is used to find the gaps between the tokens. Otherwise, it is used to\n",
    "find the tokens themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', 'wasn', 't', 'that', 'quick', 'and', 'he', 'couldn', 't', 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "# pattern to identify tokens themselves\n",
    "TOKEN_PATTERN = r'\\w+'\n",
    "regex_wt = nltk.RegexpTokenizer(pattern=TOKEN_PATTERN,gaps=False)\n",
    "words = regex_wt.tokenize(sentence)\n",
    "print (words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', \"wasn't\", 'that', 'quick', 'and', 'he', \"couldn't\", 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "# pattern to identify gaps in tokens\n",
    "GAP_PATTERN = r'\\s+'\n",
    "regex_wt = nltk.RegexpTokenizer(pattern=GAP_PATTERN,gaps=True)\n",
    "words = regex_wt.tokenize(sentence)\n",
    "print (words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 3), (4, 9), (10, 13), (14, 20), (21, 25), (26, 31), (32, 35), (36, 38), (39, 47), (48, 51), (52, 55), (56, 60)]\n",
      "['The', 'brown', 'fox', \"wasn't\", 'that', 'quick', 'and', 'he', \"couldn't\", 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "# get start and end indices of each token and then print them\n",
    "word_indices = list(regex_wt.span_tokenize(sentence))\n",
    "print (word_indices)\n",
    "print([sentence[start:end] for start, end in word_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The WordPunktTokenizer uses the pattern\n",
    "r'\\w+|[^\\w\\s]+' to tokenize sentences into independent alphabetic and non-alphabetic\n",
    "tokens. The WhitespaceTokenizer tokenizes sentences into words based on whitespaces\n",
    "like tabs, newlines, and spaces ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', 'wasn', \"'\", 't', 'that', 'quick', 'and', 'he', 'couldn', \"'\", 't', 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "wordpunkt_wt = nltk.WordPunctTokenizer()\n",
    "words = wordpunkt_wt.tokenize(sentence)\n",
    "print (words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'brown', 'fox', \"wasn't\", 'that', 'quick', 'and', 'he', \"couldn't\", 'win', 'the', 'race']\n"
     ]
    }
   ],
   "source": [
    "whitespace_wt = nltk.WhitespaceTokenizer()\n",
    "words = whitespace_wt.tokenize(sentence)\n",
    "print( words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Normalization\n",
    "Text normalization is defined as a process that consists of a series of steps that\n",
    "should be followed to wrangle, clean, and standardize textual data into a form that\n",
    "could be consumed by other NLP and analytics systems and applications as input.\n",
    "Often tokenization itself also is a part of text normalization. Besides tokenization,\n",
    "various other techniques include cleaning text, case conversion, correcting spellings,\n",
    "removing stopwords and other unnecessary terms, stemming, and lemmatization. Text\n",
    "normalization is also often called text cleansing or wrangling ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from pprint import pprint\n",
    "corpus = [\"The brown fox wasn't that quick and he couldn't win the race\",\n",
    "\"Hey that's a great deal! I just bought a phone for $199\",\n",
    "\"@@You'll (learn) a **lot** in the book. Python is an amazing language !@@\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Text\n",
    "Often the textual data we want to use or analyze contains a lot of extraneous and\n",
    "unnecessary tokens and characters that should be removed before performing any\n",
    "further operations like tokenization or other normalization techniques. This includes\n",
    "extracting out meaningful text from data sources like HTML data, which consists of\n",
    "unnecessary HTML tags, or even data from XML and JSON feeds. There are many ways\n",
    "to parse and clean this data to remove unnecessary tags. You can use functions like\n",
    "clean_html() from nltk or even the BeautifulSoup library to parse HTML data. You can\n",
    "also use your own custom logic, including regexes, xpath, and the lxml library, to parse\n",
    "through XML data. And getting data from JSON is substantially easier because it has\n",
    "definite key-value annotations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing Text\n",
    "Usually, we tokenize text before or after removing unnecessary characters and symbols\n",
    "from the data. This choice depends on the problem you are trying to solve and the data\n",
    "you are dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    word_tokens = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['The',\n",
      "   'brown',\n",
      "   'fox',\n",
      "   'was',\n",
      "   \"n't\",\n",
      "   'that',\n",
      "   'quick',\n",
      "   'and',\n",
      "   'he',\n",
      "   'could',\n",
      "   \"n't\",\n",
      "   'win',\n",
      "   'the',\n",
      "   'race']],\n",
      " [['Hey', 'that', \"'s\", 'a', 'great', 'deal', '!'],\n",
      "  ['I', 'just', 'bought', 'a', 'phone', 'for', '$', '199']],\n",
      " [['@',\n",
      "   '@',\n",
      "   'You',\n",
      "   \"'ll\",\n",
      "   '(',\n",
      "   'learn',\n",
      "   ')',\n",
      "   'a',\n",
      "   '**lot**',\n",
      "   'in',\n",
      "   'the',\n",
      "   'book',\n",
      "   '.'],\n",
      "  ['Python', 'is', 'an', 'amazing', 'language', '!'],\n",
      "  ['@', '@']]]\n"
     ]
    }
   ],
   "source": [
    "token_list = [tokenize_text(text)for text in corpus]\n",
    "pprint(token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Special Characters\n",
    "\n",
    "One important task in text normalization involves removing unnecessary and special\n",
    "characters. These may be special symbols or even punctuation that occurs in sentences.\n",
    "This step is often performed before or after tokenization. The main reason for doing so is\n",
    "because often punctuation or special characters do not have much significance when we\n",
    "analyze the text and utilize it for extracting features or information based on NLP and ML.\n",
    "We will implement both types of special characters removal, before and after tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_characters_after_tokenization(tokens):\n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    filtered_tokens = filter(None, [pattern.sub('', token) for token in tokens])\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<filter at 0x15e72be2978>,\n",
       " <filter at 0x15e72be2b38>,\n",
       " <filter at 0x15e72be2e10>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_list_1 = [filter(None,[remove_characters_after_tokenization(tokens)\n",
    "                                for tokens in sentence_tokens])\n",
    "                   for sentence_tokens in token_list]\n",
    "filtered_list_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_characters_before_tokenization(sentence,keep_apostrophes=False):\n",
    "    sentence = sentence.strip()\n",
    "    if keep_apostrophes:\n",
    "        PATTERN = r'[?|$|&|*|%|@|(|)|~]' # add other characters here to remove them\n",
    "        filtered_sentence = re.sub(PATTERN, r'', sentence)\n",
    "    else:\n",
    "        PATTERN = r'[^a-zA-Z0-9 ]' # only extract alpha-numeric characters\n",
    "        filtered_sentence = re.sub(PATTERN, r'', sentence)\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The brown fox wasnt that quick and he couldnt win the race', 'Hey thats a great deal I just bought a phone for 199', 'Youll learn a lot in the book Python is an amazing language ']\n"
     ]
    }
   ],
   "source": [
    "filtered_list_2 = [remove_characters_before_tokenization(sentence) for sentence in corpus]\n",
    "print (filtered_list_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"The brown fox wasn't that quick and he couldn't win the race\", \"Hey that's a great deal! I just bought a phone for 199\", \"You'll learn a lot in the book. Python is an amazing language !\"]\n"
     ]
    }
   ],
   "source": [
    "cleaned_corpus = [remove_characters_before_tokenization(sentence,keep_apostrophes=True) for sentence in corpus]\n",
    "print(cleaned_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expanding Contractions\n",
    "Contractions are shortened version of words or syllables. They exist in either written or\n",
    "spoken forms. Shortened versions of existing words are created by removing specific\n",
    "letters and sounds. In case of English contractions, they are often created by removing\n",
    "one of the vowels from the word. Examples would be is not to isn’t and will not to won’t ,\n",
    "where you can notice the apostrophe being used to denote the contraction and some\n",
    "of the vowels and other letters being removed.<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contractions import CONTRACTION_MAP\n",
    "\n",
    "def expand_contractions(sentence, contraction_mapping):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_sentence = contractions_pattern.sub(expand_match, sentence)\n",
    "    return expanded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The brown fox was not that quick and he could not win the race', 'Hey that is a great deal! I just bought a phone for 199', 'You will learn a lot in the book. Python is an amazing language !']\n"
     ]
    }
   ],
   "source": [
    "expanded_corpus = [expand_contractions(sentence, CONTRACTION_MAP) \n",
    "                    for sentence in cleaned_corpus]    \n",
    "print (expanded_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Conversions\n",
    "These are lowercase and uppercase conversions, where a\n",
    "body of text is converted completely to lowercase or uppercase. There are other forms\n",
    "also, such as sentence case or proper case. Lowercase is a form where all the letters of the\n",
    "text are small letters, and in uppercase they are all capitalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the brown fox wasn't that quick and he couldn't win the race\n"
     ]
    }
   ],
   "source": [
    "# lower case\n",
    "print (corpus[0].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE BROWN FOX WASN'T THAT QUICK AND HE COULDN'T WIN THE RACE\n"
     ]
    }
   ],
   "source": [
    "# upper case\n",
    "print (corpus[0].upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stopwords\n",
    "Stopwords , sometimes written stop words , are words that have little or no significance.\n",
    "They are usually removed from text during processing so as to retain words having\n",
    "maximum significance and context.<br>\n",
    "\n",
    "Words like a, the , me , and so on are stopwords. There is no universal or\n",
    "exhaustive list of stopwords. Each domain or language may have its own set of stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stopwords\n",
    "def remove_stopwords(tokens):\n",
    "    stopword_list = nltk.corpus.stopwords.words('english')\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_corpus_tokens = [tokenize_text(text)for text in expanded_corpus] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['The', 'brown', 'fox', 'quick', 'could', 'win', 'race']], [['Hey', 'great', 'deal', '!'], ['I', 'bought', 'phone', '199']], [['You', 'learn', 'lot', 'book', '.'], ['Python', 'amazing', 'language', '!']]]\n"
     ]
    }
   ],
   "source": [
    "filtered_list_3 =  [[remove_stopwords(tokens) \n",
    "                        for tokens in sentence_tokens] \n",
    "                        for sentence_tokens in expanded_corpus_tokens]\n",
    "print (filtered_list_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correcting Words\n",
    "One of the main challenges faced in text normalization is the presence of incorrect words\n",
    "in the text. The definition of incorrect here covers words that have spelling mistakes as\n",
    "well as words with several letters repeated that do not contribute much to its overall\n",
    "significance. To illustrate some examples, the word finally could be mistakenly written as\n",
    "fianlly , or someone expressing intense emotion could write it as finalllllyyyyyy . The main\n",
    "objective here would be to standardize different forms of these words to the correct form\n",
    "so that we do not end up losing vital information from different tokens in the text. This\n",
    "section covers dealing with repeated characters as well as correcting spellings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correcting Repeating Characters\n",
    "The first step in our algorithm would be to identify repeated characters in a word\n",
    "using a regex pattern and then use a substitution to remove the characters one by one.\n",
    "Consider the word finalllyyy from the earlier example. The pattern r'(\\w*)(\\w)\\2(\\w*)'\n",
    "can be used to identify characters that occur twice among other characters in the\n",
    "word, and in each step we will try to eliminate one of the repeated characters using a\n",
    "substitution for the match by utilizing the regex match groups (groups 1, 2, and 3) using\n",
    "the pattern r’\\1\\2\\3’ and then keep iterating through this process till no repeated\n",
    "characters remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1 Word: finalllyy\n",
      "Step: 2 Word: finallly\n",
      "Step: 3 Word: finally\n",
      "Step: 4 Word: finaly\n",
      "Final word: finaly\n"
     ]
    }
   ],
   "source": [
    "old_word = 'finalllyyy'\n",
    "repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "match_substitution = r'\\1\\2\\3'\n",
    "step = 1\n",
    "while True:\n",
    "    # remove one repeated character\n",
    "    new_word = repeat_pattern.sub(match_substitution,old_word)\n",
    "    if new_word != old_word:\n",
    "        print ('Step: {} Word: {}'.format(step, new_word))\n",
    "        step += 1 # update step\n",
    "        # update old word to last substituted state\n",
    "        old_word = new_word\n",
    "        continue\n",
    "    else:\n",
    "        print (\"Final word:\", new_word)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing repeated characters\n",
    "sample_sentence = 'My schooool is realllllyyy amaaazingggg'\n",
    "sample_sentence_tokens = tokenize_text(sample_sentence)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'school', 'is', 'really', 'amazing']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def remove_repeated_characters(tokens):\n",
    "    repeat_pattern = re.compile(r'(\\w*)(\\w)\\2(\\w*)')\n",
    "    match_substitution = r'\\1\\2\\3'\n",
    "    def replace(old_word):\n",
    "        if wordnet.synsets(old_word):\n",
    "            return old_word\n",
    "        new_word = repeat_pattern.sub(match_substitution, old_word)\n",
    "        return replace(new_word) if new_word != old_word else new_word\n",
    "            \n",
    "    correct_tokens = [replace(word) for word in tokens]\n",
    "    return correct_tokens\n",
    "\n",
    "print (remove_repeated_characters(sample_sentence_tokens)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_match(match): #\"\"\"Spell-correct word in match,and preserve proper upper/lower/title case.\"\"\"\n",
    "    word = match.group()\n",
    "def case_of(text): #\"\"\"Return the case-function appropriatefor text: upper, lower, title, or just str.:\"\"\"\n",
    "    return (str.upper if text.isupper() else\n",
    "    str.lower if text.islower()         else\n",
    "    str.title if text.istitle()         else \n",
    "    str)\n",
    "    return case_of(word)(correct(word.lower()))\n",
    "\n",
    "def correct_text_generic(text): #\"\"\"Correct all the words within a text,returning the corrected text.\"\"\"\n",
    "    return re.sub('[a-zA-Z]+', correct_match, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_text_generic('fianlly')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "Word stems are also often known as the base form of a\n",
    "word, and we can create new words by attaching affixes to them in a process known as\n",
    "inflection . The reverse of this is obtaining the base form of a word from its inflected form,\n",
    "and this is known as stemming .<br>\n",
    "Consider the word JUMP . You can add affixes to it and form new words like JUMPS ,\n",
    "JUMPED , and JUMPING . In this case, the base word JUMP is the word stem.\n",
    "![](https://media.springernature.com/lw785/springer-static/image/chp%3A10.1007%2F978-1-4842-2388-8_3/MediaObjects/427287_1_En_3_Fig1_HTML.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jump jump jump\n"
     ]
    }
   ],
   "source": [
    "# porter stemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "\n",
    "print (ps.stem('jumping'), ps.stem('jumps'), ps.stem('jumped'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lie\n"
     ]
    }
   ],
   "source": [
    "print (ps.stem('lying'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strang\n"
     ]
    }
   ],
   "source": [
    "print (ps.stem('strange'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LancasterStemmer\n",
    "The Lancaster stemmer is based on the Lancaster stemming algorithm, also often\n",
    "known as the Paice/Husk stemmer, invented by Chris D. Paice. This stemmer is an iterative\n",
    "stemmer that has over 120 rules specifying specific removal or replacement for affixes to\n",
    "obtain the word stems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jump jump jump\n"
     ]
    }
   ],
   "source": [
    "# lancaster stemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "ls = LancasterStemmer()\n",
    "\n",
    "print (ls.stem('jumping'), ls.stem('jumps'), ls.stem('jumped'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lying\n"
     ]
    }
   ],
   "source": [
    "print (ls.stem('lying'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strange\n"
     ]
    }
   ],
   "source": [
    "print (ls.stem('strange'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RegexpStemmer\n",
    "The RegexpStemmer uses regular expressions to identify the morphological\n",
    "affixes in words, and any part of the string matching the same is removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jump jump jump\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "rs = RegexpStemmer('ing$|s$|ed$', min=4)\n",
    "\n",
    "print (rs.stem('jumping'), rs.stem('jumps'), rs.stem('jumped'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ly\n"
     ]
    }
   ],
   "source": [
    "print (rs.stem('lying'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strange\n"
     ]
    }
   ],
   "source": [
    "print (rs.stem('strange'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SnowballStemmer\n",
    "SnowballStemmer , which supports\n",
    "stemming in 16 different languages besides English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supported Languages: ('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n"
     ]
    }
   ],
   "source": [
    "# snowball stemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "ss = SnowballStemmer(\"german\")\n",
    "\n",
    "print ('Supported Languages:', SnowballStemmer.languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'autobahn'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# autobahnen -> cars\n",
    "# autobahn -> car\n",
    "ss.stem('autobahnen')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spring'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# springen -> jumping\n",
    "# spring -> jump\n",
    "ss.stem('springen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "The process of lemmatization is very similar to stemming—you remove word affixes to\n",
    "get to a base form of the word. But in this case, this base form is also known as the root\n",
    "word , but not the root stem . The difference is that the root stem may not always be a\n",
    "lexicographically correct word; that is, it may not be present in the dictionary. The root\n",
    "word, also known as the lemma , will always be present in the dictionary.<br>\n",
    "\n",
    "\n",
    "The lemmatization process is considerably slower than stemming because an\n",
    "additional step is involved where the root form or lemma is formed by removing the affix\n",
    "from the word if and only if the lemma is present in the dictionary. The nltk package has\n",
    "a robust lemmatization module that uses WordNet and the word’s syntax and semantics,\n",
    "like part of speech and context, to get the root word or lemma. Remember parts of speech\n",
    "from Chapter 1 ? There were mainly three entities—nouns, verbs, and adjectives—that\n",
    "occur most frequently in natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "car\n",
      "men\n"
     ]
    }
   ],
   "source": [
    "# lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "# lemmatize nouns\n",
    "print (wnl.lemmatize('cars', 'n'))\n",
    "print (wnl.lemmatize('men', 'n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "eat\n"
     ]
    }
   ],
   "source": [
    "# lemmatize verbs\n",
    "print (wnl.lemmatize('running', 'v'))\n",
    "print (wnl.lemmatize('ate', 'v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sad\n",
      "fancy\n"
     ]
    }
   ],
   "source": [
    "# lemmatize adjectives\n",
    "print (wnl.lemmatize('saddest', 'a'))\n",
    "print (wnl.lemmatize('fancier', 'a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ate\n",
      "fancier\n"
     ]
    }
   ],
   "source": [
    "# ineffective lemmatization\n",
    "print (wnl.lemmatize('ate', 'n'))\n",
    "print (wnl.lemmatize('fancier', 'v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Text Syntax and Structure\n",
    "we will look and implement some of the concepts and\n",
    "techniques that are used for understanding text syntax and structure. This is extremely\n",
    "useful in NLP and is usually done after text processing and normalization . We will focus\n",
    "on implementing the following techniques:\n",
    "- Parts of speech (POS) tagging\n",
    "- Shallow parsing\n",
    "- Dependency-based parsing\n",
    "- Constituency-based parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Machine Learning Concepts\n",
    "We will be implementing and training some of our own taggers in the following section\n",
    "using corpora and also leverage existing pre-built taggers. There are some important\n",
    "concepts related to analytics and ML that you must know in order to better understand\n",
    "the implementations:\n",
    "- **Data preparation :** Usually consists of pre-processing the data before extracting features and training\n",
    "- **Feature extraction :** The process of extracting useful features from raw data that are used to train machine learning models\n",
    "- **Features :** Various useful attributes of the data (examples could be age, weight, and so on for personal data)\n",
    "- **Training data :** A set of data points used to train a model\n",
    "- **Testing/validation data :** A set of data points on which a pretrained model is tested and evaluated to see how well it performs\n",
    "- **Model :** Built using a combination of data/features and a machine learning algorithm that could be supervised or unsupervised\n",
    "- **Accuracy :** How well the model predicts something (also has otherdetailed evaluation metrics like precision, recall, and F1-score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parts of Speech (POS) Tagging\n",
    "Parts of speech (POS) are specific lexical categories to which words are assigned based on their\n",
    "syntactic context and role.the main\n",
    "POS being noun, verb, adjective, and adverb. The process of classifying and labeling POS tags\n",
    "for words called parts of speech tagging or POS tagging . POS tags are used to annotate words\n",
    "and depict their POS, which is really helpful when we need to use the same annotated text\n",
    "later in NLP-based applications because we can filter by specific parts of speech and utilize\n",
    "that information to perform specific analysis, such as narrowing down upon nouns and seeing\n",
    "which ones are the most prominent, word sense disambiguation, and grammar analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DET'), ('brown', 'ADJ'), ('fox', 'NOUN'), ('was', 'VERB'), (\"n't\", 'ADV'), ('that', 'ADP'), ('quick', 'ADJ'), ('and', 'CONJ'), ('he', 'PRON'), ('could', 'VERB'), (\"n't\", 'ADV'), ('win', 'VERB'), ('the', 'DET'), ('race', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "entence = 'The brown fox is quick and he is jumping over the lazy dog'\n",
    "\n",
    "\n",
    "# recommended tagger based on PTB\n",
    "import nltk\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tagged_sent = nltk.pos_tag(tokens, tagset='universal')\n",
    "print (tagged_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Your Own POS Taggers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# building your own tagger\n",
    "\n",
    "# preparing the data\n",
    "from nltk.corpus import treebank\n",
    "data = treebank.tagged_sents()\n",
    "train_data = data[:3500]\n",
    "test_data = data[3500:]\n",
    "print (train_data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1454158195372253\n",
      "[('The', 'NN'), ('brown', 'NN'), ('fox', 'NN'), ('was', 'NN'), (\"n't\", 'NN'), ('that', 'NN'), ('quick', 'NN'), ('and', 'NN'), ('he', 'NN'), ('could', 'NN'), (\"n't\", 'NN'), ('win', 'NN'), ('the', 'NN'), ('race', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"the DefaultTagger , which inherits from the\n",
    "SequentialBackoffTagger base class and assigns the same user input POS tag to each\n",
    "word. This may seem really naïve, but it is an excellent way to form a baseline POS tagger\n",
    "and improve upon it:\"\"\"\n",
    "\n",
    "# default tagger\n",
    "from nltk.tag import DefaultTagger\n",
    "dt = DefaultTagger('NN')\n",
    "\n",
    "print (dt.evaluate(test_data))\n",
    "\n",
    "print (dt.tag(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24039113176493368\n",
      "[('The', 'NN'), ('brown', 'NN'), ('fox', 'NN'), ('was', 'NNS'), (\"n't\", 'NN'), ('that', 'NN'), ('quick', 'NN'), ('and', 'NN'), ('he', 'NN'), ('could', 'MD'), (\"n't\", 'NN'), ('win', 'NN'), ('the', 'NN'), ('race', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# regex tagger\n",
    "from nltk.tag import RegexpTagger\n",
    "# define regex tag patterns\n",
    "patterns = [\n",
    "        (r'.*ing$', 'VBG'),               # gerunds\n",
    "        (r'.*ed$', 'VBD'),                # simple past\n",
    "        (r'.*es$', 'VBZ'),                # 3rd singular present\n",
    "        (r'.*ould$', 'MD'),               # modals\n",
    "        (r'.*\\'s$', 'NN$'),               # possessive nouns\n",
    "        (r'.*s$', 'NNS'),                 # plural nouns\n",
    "        (r'^-?[0-9]+(.[0-9]+)?$', 'CD'),  # cardinal numbers\n",
    "        (r'.*', 'NN')                     # nouns (default) ... \n",
    "]\n",
    "rt = RegexpTagger(patterns)\n",
    "\n",
    "print (rt.evaluate(test_data))\n",
    "print (rt.tag(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **UnigramTagger** , **BigramTagger** , and **TrigramTagger** are\n",
    "classes that inherit from the base class **NGramTagger** , which itself inherits from the\n",
    "**ContextTagger class** , which inherits from the **SequentialBackoffTagger class** . We will\n",
    "use train_data as training data to train the n-gram taggers based on sentence tokens and\n",
    "their POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tag import BigramTagger\n",
    "from nltk.tag import TrigramTagger\n",
    "\n",
    "uni_gram = UnigramTagger(train_data)\n",
    "bi_gram= BigramTagger(train_data)\n",
    "tri_gram= TrigramTagger(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8607803272340013\n"
     ]
    }
   ],
   "source": [
    "# testing performance of unigram tagger\n",
    "print(uni_gram.evaluate(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('brown', None), ('fox', None), ('was', 'VBD'), (\"n't\", 'RB'), ('that', 'IN'), ('quick', 'JJ'), ('and', 'CC'), ('he', 'PRP'), ('could', 'MD'), (\"n't\", 'RB'), ('win', 'VB'), ('the', 'DT'), ('race', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "print (uni_gram.tag(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13466937748087907\n"
     ]
    }
   ],
   "source": [
    "# testing performance of bigram tagger\n",
    "print(bi_gram.evaluate(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('brown', None), ('fox', None), ('was', None), (\"n't\", None), ('that', None), ('quick', None), ('and', None), ('he', None), ('could', None), (\"n't\", None), ('win', None), ('the', None), ('race', None)]\n"
     ]
    }
   ],
   "source": [
    "print (bi_gram.tag(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08064672281924679\n"
     ]
    }
   ],
   "source": [
    "# testing performance of trigram tagger\n",
    "print(tri_gram.evaluate(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('brown', None), ('fox', None), ('was', None), (\"n't\", None), ('that', None), ('quick', None), ('and', None), ('he', None), ('could', None), (\"n't\", None), ('win', None), ('the', None), ('race', None)]\n"
     ]
    }
   ],
   "source": [
    "print (tri_gram.tag(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation:\n",
    "The preceding output clearly shows that we obtain 86 percent accuracy on the test\n",
    "set using UnigramTagger tagger alone, which is really good compared to our last tagger.\n",
    "The None tag indicates the tagger was unable to tag that word, the reason being that it was\n",
    "unable to get a similar token in the training data. Accuracies of the bigram and trigram\n",
    "models are far less because it is not always the case that the same bigrams and trigrams it\n",
    "had observed in the training data will also be present in the same way in the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9094781682641108\n",
      "[('The', 'DT'), ('brown', 'NN'), ('fox', 'NN'), ('was', 'VBD'), (\"n't\", 'RB'), ('that', 'RB'), ('quick', 'JJ'), ('and', 'CC'), ('he', 'PRP'), ('could', 'MD'), (\"n't\", 'RB'), ('win', 'VB'), ('the', 'DT'), ('race', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "def combined_tagger(train_data, taggers, backoff=None):\n",
    "    for tagger in taggers:\n",
    "        backoff = tagger(train_data, backoff=backoff)\n",
    "    return backoff\n",
    "\n",
    "ct = combined_tagger(train_data=train_data, \n",
    "                     taggers=[UnigramTagger, BigramTagger, TrigramTagger],\n",
    "                     backoff=rt)\n",
    "\n",
    "print (ct.evaluate(test_data) )       \n",
    "print (ct.tag(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation:\n",
    "We now obtain an accuracy of 91 percent on the test data, which is excellent. Also we\n",
    "see that this new tagger is able to successfully tag all the tokens in our sample sentence\n",
    "(even though a couple of them are not correct, like brown should be an adjective)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9306806079969019\n",
      "[('The', 'DT'), ('brown', 'JJ'), ('fox', 'NN'), ('was', 'VBD'), (\"n't\", 'RB'), ('that', 'IN'), ('quick', 'JJ'), ('and', 'CC'), ('he', 'PRP'), ('could', 'MD'), (\"n't\", 'RB'), ('win', 'VB'), ('the', 'DT'), ('race', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify import NaiveBayesClassifier, MaxentClassifier\n",
    "from nltk.tag.sequential import ClassifierBasedPOSTagger\n",
    "\n",
    "nbt = ClassifierBasedPOSTagger(train=train_data,\n",
    "                               classifier_builder=NaiveBayesClassifier.train)\n",
    "\n",
    "# evaluate tagger on test data and sample sentence\n",
    "print (nbt.evaluate(test_data))\n",
    "\n",
    "print (nbt.tag(tokens) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shallow Parsing\n",
    "Shallow parsing , also known as light parsing or chunking , is a technique of analyzing the\n",
    "structure of a sentence to break it down into its smallest constituents (which are tokens\n",
    "such as words) and group them together into higher-level phrases. In shallow parsing,\n",
    "there is more focus on identifying these phrases or chunks rather than diving into further\n",
    "details of the internal syntax and relations inside each chunk, like we see in grammarbased\n",
    "parse trees obtained from deep parsing. The main objective of shallow parsing is to\n",
    "obtain semantically meaningful phrases and observe relations among them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Your Own Shallow Parsers\n",
    "We will use several techniques like regular expressions and tagging-based learners to build\n",
    "our own shallow parsers. As with POS tagging, we will use some training data to train our\n",
    "parsers if needed and evaluate all our parsers on some test data and also on our sample\n",
    "sentence. The treebank corpus is available in nltk with chunk annotations . We will load it\n",
    "first and prepare our training and testing datasets using the following code snippet :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shrikant\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\regexp.py:123: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return [tok for tok in self._regexp.split(text) if tok]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import treebank_chunk\n",
    "data = treebank_chunk.chunked_sents()\n",
    "train_data = data[:4000]\n",
    "test_data = data[4000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP A/DT Lorillard/NNP spokewoman/NN)\n",
      "  said/VBD\n",
      "  ,/,\n",
      "  ``/``\n",
      "  (NP This/DT)\n",
      "  is/VBZ\n",
      "  (NP an/DT old/JJ story/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print (train_data[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chinking**\n",
    "is the reverse of chunking, where we specify which specific tokens we do not want to be\n",
    "a part of any chunk and then form the necessary chunks excluding these tokens. Let us\n",
    "consider a simple sentence and use regular expressions by leveraging the RegexpParser\n",
    "class to create shallow parsers to illustrate both chunking and chinking for noun phrases :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constituency-based Parsing\n",
    "Constituent-based grammars are used to analyze and determine the constituents\n",
    "a sentence is usually composed of. Besides determining the constituents, another\n",
    "important objective is to find out the internal structure of these constituents and see how\n",
    "they link to each other. There are usually several rules for different types of phrases based\n",
    "on the type of components they can contain, and we can use them to build parse trees.\n",
    "Refer to the “Constituency Grammars” subsection under “Grammar” in the “Language\n",
    "Syntax and Structure” section from Chapter 1 if you need to refresh your memory and\n",
    "look at some examples of sample parse trees.\n",
    "\n",
    "There are various types of parsing algorithms, including the following:\n",
    "- Recursive Descent parsing\n",
    "- Shift Reduce parsing\n",
    "- Chart parsing\n",
    "- Bottom-up parsing\n",
    "- Top-down parsing\n",
    "- PCFG parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Shift Reduce parsing:** follows a bottom-up parsing approach where it finds sequences\n",
    "of tokens (words/phrases) that correspond to the righthand side of grammar productions\n",
    "and then replaces it with the lefthand side for that rule. This process continues until the\n",
    "whole sentence is reduced to give us a parse tree.\n",
    "**Chart parsing:** uses dynamic programming , which stores intermediate results and\n",
    "reuses them when needed to get significant efficiency gains. In this case, chart parsers\n",
    "store partial solutions and look them up when needed to get to the complete solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
